{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as th\n",
    "import torch.nn as nn\n",
    "import timeit\n",
    "import statistics\n",
    "from branchNetwork.BranchLayer import BranchLayer as BL # Adjust the import according to your project structure\n",
    "from branchNetwork.BranchLayerMM import BranchLayer as BLMM # Adjust the import accordingly as BranchLayerMM\n",
    "\n",
    "# Optional: If you use GPU and want to measure GPU memory\n",
    "# th.cuda.reset_peak_memory_stats()\n",
    "# th.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_branch_layer(layer_class, branch_params, num_trials=1000):\n",
    "    durations = []\n",
    "    memory_usages = []\n",
    "    grad_checks = []\n",
    "\n",
    "    for _ in range(num_trials):\n",
    "        branch_layer = layer_class(**branch_params)\n",
    "        branch_layer.train()  # Ensure the layer is in training mode for gradient checks\n",
    "\n",
    "        x = th.randn(5, branch_params['n_in'], requires_grad=True)\n",
    "        start_time = timeit.default_timer()\n",
    "\n",
    "        # Forward pass\n",
    "        out = branch_layer(x)\n",
    "        assert out.shape == (5, branch_params['n_b'], branch_params['n_next_h']), \"Output shape mismatch\"\n",
    "\n",
    "        # Backward pass for gradient check\n",
    "        out.sum().backward()\n",
    "\n",
    "        # Record the end time and compute the duration\n",
    "        durations.append(timeit.default_timer() - start_time)\n",
    "\n",
    "        # Memory usage check (if on GPU)\n",
    "        if th.cuda.is_available():\n",
    "            memory_usages.append(th.cuda.memory_allocated() / (1024 ** 2))  # Memory in MB\n",
    "\n",
    "        # Gradient check\n",
    "        grad_checks.append(x.grad is not None)\n",
    "\n",
    "        # Reset gradients\n",
    "        branch_layer.zero_grad()\n",
    "        if x.grad is not None:\n",
    "            x.grad.zero_()\n",
    "\n",
    "    # Calculate average time and memory usage\n",
    "    avg_duration = statistics.mean(durations)\n",
    "    std_duration = statistics.stdev(durations)\n",
    "    avg_memory_usage = statistics.mean(memory_usages) if memory_usages else 0\n",
    "    std_memory_usage = statistics.stdev(memory_usages) if memory_usages else 0\n",
    "\n",
    "    all_gradients_ok = all(grad_checks)\n",
    "\n",
    "    print(f\"Layer: {layer_class.__name__}\")\n",
    "    print(f\"Average Duration: {avg_duration:.4f} sec, Std Dev: {std_duration:.4f} sec\")\n",
    "    print(f\"Average Memory Usage: {avg_memory_usage:.2f} MB, Std Dev: {std_memory_usage:.2f} MB\")\n",
    "    print(f\"All gradients computed: {all_gradients_ok}\\n\")\n",
    "\n",
    "    return avg_duration, avg_memory_usage, all_gradients_ok\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gpu_test_branch_layer(layer_class_1, layer_class_2, branch_params, num_trials=10, device='cpu'):\n",
    "    durations = []\n",
    "    memory_usages = []\n",
    "    outputs_equivalent = []\n",
    "\n",
    "    x = th.randn(5, branch_params['n_in'], requires_grad=True).to(device)\n",
    "    expected_output = None\n",
    "\n",
    "    for _ in range(num_trials):\n",
    "        # Instantiate layers and move to specified device\n",
    "        layer1 = layer_class_1(**branch_params).to(device)\n",
    "        layer2 = layer_class_2(**branch_params).to(device)\n",
    "\n",
    "        layer1.train()\n",
    "        layer2.train()\n",
    "\n",
    "        start_time = timeit.default_timer()\n",
    "\n",
    "        # Forward pass for both layers\n",
    "        out1 = layer1(x)\n",
    "        out2 = layer2(x)\n",
    "        print(out1)\n",
    "        print(out2)\n",
    "\n",
    "        # Check for equivalence of outputs\n",
    "        outputs_equivalent.append(th.allclose(out1, out2, atol=1e-6))\n",
    "\n",
    "        # Backward pass for gradient checks (on one to save time)\n",
    "        out1.sum().backward()\n",
    "\n",
    "        # Record time and reset gradients\n",
    "        durations.append(timeit.default_timer() - start_time)\n",
    "        layer1.zero_grad()\n",
    "        x.grad.zero_()\n",
    "\n",
    "        # Memory usage check (if on GPU)\n",
    "        if device == 'cuda':\n",
    "            memory_usages.append(th.cuda.memory_allocated() / (1024 ** 2))  # Memory in MB\n",
    "\n",
    "    # Calculate statistics\n",
    "    avg_duration = statistics.mean(durations)\n",
    "    std_duration = statistics.stdev(durations)\n",
    "    avg_memory_usage = statistics.mean(memory_usages) if memory_usages else 0\n",
    "    std_memory_usage = statistics.stdev(memory_usages) if memory_usages else 0\n",
    "    output_consistency = all(outputs_equivalent)\n",
    "\n",
    "    print(f\"Layer comparison: {layer_class_1.__name__} vs {layer_class_2.__name__}\")\n",
    "    print(f\"Average Duration: {avg_duration:.4f} sec, Std Dev: {std_duration:.4f} sec\")\n",
    "    print(f\"Average Memory Usage: {avg_memory_usage:.2f} MB, Std Dev: {std_memory_usage:.2f} MB\")\n",
    "    print(f\"Output consistency across all trials: {output_consistency}\\n\")\n",
    "\n",
    "    return avg_duration, avg_memory_usage, output_consistency\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: BranchLayer\n",
      "Average Duration: 0.0067 sec, Std Dev: 0.0005 sec\n",
      "Average Memory Usage: 0.00 MB, Std Dev: 0.00 MB\n",
      "All gradients computed: True\n",
      "\n",
      "Layer: BranchLayer\n",
      "Average Duration: 0.0028 sec, Std Dev: 0.0023 sec\n",
      "Average Memory Usage: 0.00 MB, Std Dev: 0.00 MB\n",
      "All gradients computed: True\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.002842916499823332, 0, True)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set parameters common to both layer types\n",
    "branch_params = {\n",
    "    'n_in': 800,\n",
    "    'n_npb': 40,\n",
    "    'n_b': 20,\n",
    "    'n_next_h': 400\n",
    "}\n",
    "\n",
    "# Run test for BranchLayer\n",
    "test_branch_layer(BL, branch_params)\n",
    "\n",
    "# Run test for BranchLayerMM\n",
    "test_branch_layer(BLMM, branch_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 2.2933e-01, -2.4450e+00, -1.2587e-01,  8.2587e-01],\n",
      "         [ 3.4846e-01,  1.8635e+00, -3.8021e-01, -2.0549e-01]],\n",
      "\n",
      "        [[-5.0282e-01,  3.8076e-01,  6.8913e-01, -1.3163e-01],\n",
      "         [ 1.4247e+00,  6.6359e-01,  2.7320e-01, -2.4472e-01]],\n",
      "\n",
      "        [[ 2.0457e-01, -6.7910e-01,  2.5330e-01,  1.0349e-01],\n",
      "         [ 9.5158e-01,  1.1612e+00, -1.6054e-01, -1.8892e-01]],\n",
      "\n",
      "        [[ 4.6132e-01,  7.5304e-01,  5.1296e-01,  2.5104e-01],\n",
      "         [ 1.6296e-03, -6.3104e-01,  2.3748e-02,  9.0217e-02]],\n",
      "\n",
      "        [[ 4.5345e-01, -8.4221e-01, -1.2751e+00,  2.6968e-01],\n",
      "         [-1.2051e+00, -1.1460e+00,  5.1721e-01,  4.0247e-01]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "tensor([[[-1.9712e+00, -4.6442e-01,  3.5482e+00, -1.6731e+00],\n",
      "         [-5.8860e-01, -6.4764e-02,  9.5958e-01,  1.5001e+00]],\n",
      "\n",
      "        [[ 3.1809e-03, -6.5825e-01,  9.3834e-01, -8.8280e-01],\n",
      "         [-6.1468e-01,  7.0424e-02, -1.8015e-01, -2.9362e-01]],\n",
      "\n",
      "        [[-1.3388e+00, -7.0561e-01,  1.3234e+00, -1.0264e+00],\n",
      "         [-5.3870e-01,  2.8700e-01, -1.7902e-01,  4.7347e-01]],\n",
      "\n",
      "        [[-1.4370e+00,  6.9845e-01,  9.5328e-01, -5.5916e-01],\n",
      "         [-3.4486e-01,  8.0498e-01, -8.1637e-01, -5.3234e-01]],\n",
      "\n",
      "        [[ 4.8496e-01, -3.8197e-01, -8.2821e-01,  3.9087e-01],\n",
      "         [ 2.5923e-02,  6.5841e-01, -1.6940e+00,  1.4172e+00]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "tensor([[[ 2.0850,  3.6357,  0.8273, -1.2559],\n",
      "         [-0.9639, -0.3854,  1.5255,  0.5611]],\n",
      "\n",
      "        [[ 0.2265,  0.7287, -0.3013,  1.6292],\n",
      "         [ 0.3319,  0.2553,  0.4258, -0.0690]],\n",
      "\n",
      "        [[ 1.1494,  1.4064,  0.4614, -1.4508],\n",
      "         [-0.2821, -0.3507,  1.0036,  0.3494]],\n",
      "\n",
      "        [[ 1.4217, -1.1050,  0.9755, -3.3787],\n",
      "         [ 0.0933,  0.5122,  1.2428,  0.7079]],\n",
      "\n",
      "        [[ 0.6487, -0.3807,  0.3281, -1.4587],\n",
      "         [ 1.7936, -0.5663,  0.8318,  0.7333]]], grad_fn=<ViewBackward0>)\n",
      "tensor([[[-1.4774e+00, -1.4158e+00, -1.3654e+00, -2.6492e+00],\n",
      "         [ 1.2063e+00, -1.6857e-04, -1.0818e+00, -1.4267e+00]],\n",
      "\n",
      "        [[-1.0557e-01, -6.2868e-01, -8.1393e-01,  2.9996e-01],\n",
      "         [-2.5165e-02,  5.5380e-02, -3.8989e-01,  4.5994e-01]],\n",
      "\n",
      "        [[ 5.2318e-01, -7.6016e-01, -5.9469e-01, -1.1997e+00],\n",
      "         [ 1.7382e+00,  8.6909e-02, -7.0511e-01, -7.7307e-02]],\n",
      "\n",
      "        [[ 1.0513e+00, -8.3804e-02,  8.2250e-02, -1.1510e+00],\n",
      "         [ 2.0374e+00, -4.7485e-01,  1.2392e+00,  3.8656e-01]],\n",
      "\n",
      "        [[ 1.1929e+00,  1.7910e+00,  5.2847e-01,  1.1575e+00],\n",
      "         [ 7.1357e-02, -1.4939e+00,  5.9768e-01,  1.2989e+00]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "tensor([[[ 0.6949, -0.5624, -1.6912, -0.5519],\n",
      "         [-0.1455, -1.2280,  2.0779, -2.7732]],\n",
      "\n",
      "        [[-1.1605, -0.2162, -0.2342, -0.2433],\n",
      "         [-0.4768, -0.8864,  0.7707, -1.3771]],\n",
      "\n",
      "        [[ 0.3272, -0.0935, -1.1809, -0.9850],\n",
      "         [ 0.7569, -0.5319,  0.9463, -1.6105]],\n",
      "\n",
      "        [[-0.1757,  0.8496, -1.2168, -1.7573],\n",
      "         [ 2.0432,  1.7375,  0.6412, -0.5796]],\n",
      "\n",
      "        [[ 1.9515,  0.2414,  0.6898, -0.6923],\n",
      "         [ 2.4679,  0.2874,  0.2441,  1.4000]]], grad_fn=<ViewBackward0>)\n",
      "tensor([[[ 0.4722, -2.9916,  0.5156,  2.2539],\n",
      "         [-0.8669, -1.4689, -1.4846, -0.7297]],\n",
      "\n",
      "        [[-0.2109,  0.3867,  0.0720,  0.6387],\n",
      "         [ 0.0369, -0.5171,  0.2806, -0.1353]],\n",
      "\n",
      "        [[ 0.2428, -1.3782, -0.2603,  1.1541],\n",
      "         [-0.8469, -1.4529, -0.3501, -0.4294]],\n",
      "\n",
      "        [[-0.8822, -0.2168, -0.4063, -0.3447],\n",
      "         [-1.1024, -0.9496,  1.1707, -1.7308]],\n",
      "\n",
      "        [[-0.1277,  0.6464, -0.3790,  0.1193],\n",
      "         [ 0.9834,  1.7631, -0.7232,  1.4851]]], grad_fn=<ViewBackward0>)\n",
      "tensor([[[-1.0436,  1.8881,  0.8557,  0.8295],\n",
      "         [ 0.1021, -1.6437,  0.0777, -0.3390]],\n",
      "\n",
      "        [[-1.1449,  1.2894,  0.6131, -0.3430],\n",
      "         [-0.9294, -0.6228,  0.2756, -0.1328]],\n",
      "\n",
      "        [[-0.1998,  1.2820,  0.5879,  0.2337],\n",
      "         [ 0.7265,  0.0738,  0.0362, -0.1043]],\n",
      "\n",
      "        [[ 1.1587,  0.5651, -1.0985,  2.0656],\n",
      "         [ 2.2565,  2.1274, -0.3517, -0.0472]],\n",
      "\n",
      "        [[ 1.7617, -0.8620, -0.1826, -0.6563],\n",
      "         [ 1.1420,  0.6823, -0.7539, -0.0783]]], grad_fn=<ViewBackward0>)\n",
      "tensor([[[ 0.2925,  0.9012,  0.2250,  1.5523],\n",
      "         [ 2.5459,  0.1001,  0.8167, -0.7170]],\n",
      "\n",
      "        [[ 0.6481,  0.5861,  0.3107,  0.5482],\n",
      "         [ 0.3435, -0.0699, -0.0403, -0.4123]],\n",
      "\n",
      "        [[-0.4263, -0.0228,  0.3520,  0.7264],\n",
      "         [ 1.1215,  0.2639, -0.2193, -0.2353]],\n",
      "\n",
      "        [[-1.1484, -0.4364,  0.8262, -0.1150],\n",
      "         [-1.1403,  0.7177, -0.0618,  1.2287]],\n",
      "\n",
      "        [[-1.3364, -0.8434, -1.3431, -1.5111],\n",
      "         [-0.5057,  0.4031, -0.5716,  0.0714]]], grad_fn=<ViewBackward0>)\n",
      "tensor([[[-1.4216, -1.3084, -0.6065,  0.6654],\n",
      "         [ 2.9299, -0.1196, -2.2284, -0.4324]],\n",
      "\n",
      "        [[-0.8203, -0.0699, -1.3225,  0.1331],\n",
      "         [ 0.6789,  0.6236,  0.4526,  0.0071]],\n",
      "\n",
      "        [[-0.8648,  0.2102, -0.4054,  0.2940],\n",
      "         [ 1.7453, -0.5300, -1.9083,  0.3321]],\n",
      "\n",
      "        [[ 0.0298,  2.3350,  0.1304, -1.1358],\n",
      "         [ 1.1562, -0.5172, -2.8420,  0.4467]],\n",
      "\n",
      "        [[ 1.4627,  0.1914,  0.6912,  1.0389],\n",
      "         [-1.4757, -0.3936, -0.3401,  0.3082]]], grad_fn=<ViewBackward0>)\n",
      "tensor([[[-0.1524,  0.9818,  0.2358, -0.0900],\n",
      "         [-1.7209, -0.3691,  0.6403,  0.0144]],\n",
      "\n",
      "        [[ 0.3222, -0.0808, -0.0842,  0.6849],\n",
      "         [-0.4569, -0.2606,  0.4845, -0.0616]],\n",
      "\n",
      "        [[-0.3496,  0.9446,  0.9357, -0.6604],\n",
      "         [-0.2670, -0.0187,  0.1837, -0.6471]],\n",
      "\n",
      "        [[-0.2311,  1.1404,  0.1710,  0.2091],\n",
      "         [ 0.1098,  0.6243, -0.5847, -0.5979]],\n",
      "\n",
      "        [[ 0.2793, -0.3626,  0.1211, -1.6823],\n",
      "         [ 1.0328, -0.8326, -1.1045, -0.5671]]], grad_fn=<ViewBackward0>)\n",
      "tensor([[[-1.1222, -1.4639, -0.3656, -1.5018],\n",
      "         [ 0.7374, -0.3141,  0.3103, -1.4607]],\n",
      "\n",
      "        [[-0.1047, -0.6899,  0.3142,  0.4021],\n",
      "         [ 1.6926, -0.1500,  0.4551,  0.7402]],\n",
      "\n",
      "        [[-0.3529, -0.3962,  0.6365, -1.0335],\n",
      "         [ 0.3730,  0.2020, -0.7090, -0.9800]],\n",
      "\n",
      "        [[-0.4590,  0.0022,  1.1424,  0.3242],\n",
      "         [-0.7670,  0.2796, -0.5782, -0.7053]],\n",
      "\n",
      "        [[-0.5585, -0.1677,  0.9167, -0.9112],\n",
      "         [-0.6747,  0.2382, -1.0582, -2.0229]]], grad_fn=<ViewBackward0>)\n",
      "tensor([[[-0.3168, -0.2962, -1.4599, -0.4567],\n",
      "         [ 1.6891, -0.1082,  0.6031, -1.7976]],\n",
      "\n",
      "        [[ 0.7034, -0.5280, -0.2449,  0.3529],\n",
      "         [-0.0556,  0.6254, -0.1028, -0.0101]],\n",
      "\n",
      "        [[-0.5127,  0.0260, -0.5873,  0.1342],\n",
      "         [ 1.1258,  0.1964, -0.4644, -0.3514]],\n",
      "\n",
      "        [[ 0.3345,  0.4360,  1.4200, -0.6045],\n",
      "         [ 1.5016,  0.1578, -0.6394,  2.2857]],\n",
      "\n",
      "        [[-0.9065,  0.0525,  0.2566, -0.6742],\n",
      "         [ 0.0868,  0.6302, -0.5142,  1.1297]]], grad_fn=<ViewBackward0>)\n",
      "tensor([[[ 1.0162,  0.0497, -2.0456, -0.9332],\n",
      "         [-0.8586, -3.0411, -0.2213, -0.9357]],\n",
      "\n",
      "        [[ 0.8915, -0.6730,  0.1518,  0.2588],\n",
      "         [ 0.2054, -0.8902,  0.7070, -0.6283]],\n",
      "\n",
      "        [[ 0.4222,  0.0948, -0.9608,  0.7756],\n",
      "         [-0.3284, -1.2025, -0.5794, -0.8332]],\n",
      "\n",
      "        [[ 0.3164,  0.2880,  0.6390,  0.9005],\n",
      "         [-0.0780, -0.6099, -0.7929, -0.6932]],\n",
      "\n",
      "        [[ 0.3807,  0.8357, -0.3800,  0.5939],\n",
      "         [ 1.2673,  1.6621, -1.3431,  0.3212]]], grad_fn=<ViewBackward0>)\n",
      "tensor([[[ 0.3231,  0.2774,  2.6238,  0.7335],\n",
      "         [ 1.3527,  3.2825, -0.8670, -1.5923]],\n",
      "\n",
      "        [[ 0.6304, -0.6366, -0.7549, -0.0524],\n",
      "         [ 0.6635, -0.2657,  0.1372,  0.6794]],\n",
      "\n",
      "        [[-0.9472,  0.7357,  1.3383, -0.1567],\n",
      "         [ 0.7302,  2.1679, -0.4118, -1.4549]],\n",
      "\n",
      "        [[-1.3699,  1.7783,  0.4393,  0.1413],\n",
      "         [-0.0590,  0.6397,  0.6360, -2.4528]],\n",
      "\n",
      "        [[-0.2011,  1.8063, -0.0935,  0.2617],\n",
      "         [-1.4808,  0.1318, -0.4375, -0.4535]]], grad_fn=<ViewBackward0>)\n",
      "tensor([[[-1.0195, -1.0677,  0.8041,  0.5342],\n",
      "         [ 0.3007,  0.9301,  0.4547,  0.3519]],\n",
      "\n",
      "        [[ 0.2009, -0.7305,  0.2608, -0.2841],\n",
      "         [-0.0732,  0.0336,  0.7938, -0.7403]],\n",
      "\n",
      "        [[-1.0160, -1.0464, -0.0784,  0.1310],\n",
      "         [ 0.0036,  0.5568, -0.2828, -0.3697]],\n",
      "\n",
      "        [[-0.3780, -0.8995, -0.3880,  0.3344],\n",
      "         [-0.7990,  0.6851, -1.2280, -0.3252]],\n",
      "\n",
      "        [[-0.1631,  0.3394, -0.4190,  0.2518],\n",
      "         [ 0.2440,  0.0196, -1.5893, -0.4527]]], grad_fn=<ViewBackward0>)\n",
      "tensor([[[ 0.8023, -0.7396, -0.0841,  0.3391],\n",
      "         [ 2.2978, -0.9338, -0.1419, -0.5337]],\n",
      "\n",
      "        [[ 1.4055,  0.5330,  0.2206,  0.0273],\n",
      "         [ 0.3756, -0.0435, -0.2962, -1.0747]],\n",
      "\n",
      "        [[ 0.3334, -0.1085, -0.3271,  0.1837],\n",
      "         [ 0.5586, -0.2955,  0.0505, -0.2680]],\n",
      "\n",
      "        [[-0.2100,  0.3610,  0.8623,  0.1211],\n",
      "         [-1.1104,  0.4267, -0.3506,  0.0036]],\n",
      "\n",
      "        [[-0.1297,  0.0977, -0.8679, -0.3161],\n",
      "         [ 0.3401,  0.7899,  0.0226, -0.0217]]], grad_fn=<ViewBackward0>)\n",
      "tensor([[[ 1.6316,  1.0693, -0.5297,  1.0440],\n",
      "         [-0.5790,  1.3073,  0.2272, -1.0077]],\n",
      "\n",
      "        [[ 0.6915, -0.5420, -0.1222,  0.2515],\n",
      "         [ 0.5961,  0.6621,  1.0146,  0.0806]],\n",
      "\n",
      "        [[ 2.1207,  0.3637, -0.6469,  0.5930],\n",
      "         [ 0.7142,  0.4165,  0.2821, -0.9702]],\n",
      "\n",
      "        [[ 2.8304, -0.0928, -0.9581, -1.4241],\n",
      "         [ 1.1575, -0.8454,  0.5137, -1.7527]],\n",
      "\n",
      "        [[ 1.1112,  1.0591, -1.1758, -0.1164],\n",
      "         [ 1.0096, -0.7268, -1.2164,  0.2242]]], grad_fn=<ViewBackward0>)\n",
      "tensor([[[ 0.2001,  0.2549,  2.6852, -1.2307],\n",
      "         [ 0.6558, -1.6325,  0.2824, -3.1560]],\n",
      "\n",
      "        [[ 0.1141,  0.6256,  0.3187,  0.7945],\n",
      "         [-0.0167, -0.1888, -0.0887, -0.6069]],\n",
      "\n",
      "        [[ 0.8726, -0.3687,  1.5231, -0.6075],\n",
      "         [ 0.3751, -1.0439,  0.3649, -1.4723]],\n",
      "\n",
      "        [[ 1.2001,  0.7992,  1.5859,  0.1429],\n",
      "         [ 0.6661, -0.3741,  0.3355, -0.5799]],\n",
      "\n",
      "        [[ 0.4862, -1.0941, -0.3099, -1.3053],\n",
      "         [ 0.7230,  0.4223,  0.0561,  2.7412]]], grad_fn=<ViewBackward0>)\n",
      "tensor([[[ 0.2607, -0.7724,  0.2830, -0.9419],\n",
      "         [-0.3069, -1.4577, -0.9159,  0.8481]],\n",
      "\n",
      "        [[-1.1042, -0.0939, -0.1825,  0.4764],\n",
      "         [ 0.9613,  0.0330, -0.5370, -0.4728]],\n",
      "\n",
      "        [[ 0.4873, -0.4883, -0.3808, -0.0205],\n",
      "         [-0.5465, -0.5384, -0.1501,  0.4279]],\n",
      "\n",
      "        [[ 1.3958, -0.6480, -0.9110,  0.3631],\n",
      "         [ 0.4207, -0.9413,  0.3120,  0.1062]],\n",
      "\n",
      "        [[ 0.1186, -0.4285, -1.2821, -0.4683],\n",
      "         [-1.1490,  0.0525,  0.8501,  0.6579]]], grad_fn=<ViewBackward0>)\n",
      "tensor([[[-0.2728,  0.7143,  0.5175,  1.4640],\n",
      "         [ 0.2857, -1.6196,  1.0530, -3.1143]],\n",
      "\n",
      "        [[-0.2214, -0.7987, -0.4028, -0.1373],\n",
      "         [-0.3901,  0.6630,  0.5998, -0.0924]],\n",
      "\n",
      "        [[ 0.0119, -0.6107,  0.0478,  1.2872],\n",
      "         [ 0.1207, -1.5718, -0.1660, -1.7131]],\n",
      "\n",
      "        [[ 0.9639, -0.9721, -0.0395,  0.0086],\n",
      "         [-0.1087, -1.7372, -0.5413, -0.3009]],\n",
      "\n",
      "        [[ 1.8396, -1.2766, -0.5429,  2.1661],\n",
      "         [ 1.6518,  0.8437, -0.0033,  0.3571]]], grad_fn=<ViewBackward0>)\n",
      "Layer comparison: BranchLayer vs BranchLayer\n",
      "Average Duration: 0.0009 sec, Std Dev: 0.0002 sec\n",
      "Average Memory Usage: 0.00 MB, Std Dev: 0.00 MB\n",
      "Output consistency across all trials: False\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.0009005951229482889, 0, False)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Device configuration\n",
    "device = 'cuda' if th.cuda.is_available() else 'cpu'\n",
    "\n",
    "# Parameters common to both layer types\n",
    "branch_params = {\n",
    "    'n_in': 8,\n",
    "    'n_npb': 4,\n",
    "    'n_b': 2,\n",
    "    'n_next_h': 4\n",
    "}\n",
    "\n",
    "# Run test for both BranchLayer versions\n",
    "gpu_test_branch_layer(BL, BLMM, branch_params, device=device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data-science",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
